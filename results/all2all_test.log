
round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
A2A: 0.6019628047943115, REAL_BD: 1.6210641054421413 GB/s, TOTAL_BD: 1.5574051960242632 GB/s, comm_vol: 0.9375 GB
P2P: 0.5408248901367188, REAL_BD: 1.756594976236742 GB/s, TOTAL_BD: 1.7334631173557915 GB/s, comm_vol: 0.9375 GB
AGD: 1.115638256072998, REAL_BD: 16.151523893048147 GB/s, TOTAL_BD: 0.8403261495352108 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
A2A: 1.1530179977416992, REAL_BD: 1.6595359491674668 GB/s, TOTAL_BD: 1.6261671575572754 GB/s, comm_vol: 1.875 GB
P2P: 1.1076953411102295, REAL_BD: 1.7113231902516737 GB/s, TOTAL_BD: 1.6927036978603796 GB/s, comm_vol: 1.875 GB
AGD: 0.4540987014770508, REAL_BD: 17.032174785147866 GB/s, TOTAL_BD: 4.129058272796578 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
A2A: 2.2504870891571045, REAL_BD: 1.6899869845621347 GB/s, TOTAL_BD: 1.6663059379756415 GB/s, comm_vol: 3.75 GB
P2P: 2.232926368713379, REAL_BD: 1.6974037967962385 GB/s, TOTAL_BD: 1.679410504772159 GB/s, comm_vol: 3.75 GB
AGD: 0.887537956237793, REAL_BD: 17.339799494053697 GB/s, TOTAL_BD: 4.225171412269476 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
A2A: 4.472095966339111, REAL_BD: 1.6951121019738262 GB/s, TOTAL_BD: 1.677065979006607 GB/s, comm_vol: 7.5 GB
P2P: 4.496685743331909, REAL_BD: 1.6839052064088602 GB/s, TOTAL_BD: 1.6678950738600482 GB/s, comm_vol: 7.5 GB
AGD: 1.801332712173462, REAL_BD: 17.04339639505832 GB/s, TOTAL_BD: 4.163583967200934 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
A2A: 8.976181745529175, REAL_BD: 1.6892422127442686 GB/s, TOTAL_BD: 1.6710891585357157 GB/s, comm_vol: 15.0 GB
P2P: 8.822652816772461, REAL_BD: 1.7150442941743544 GB/s, TOTAL_BD: 1.7001689074156905 GB/s, comm_vol: 15.0 GB
AGD: 3.6104514598846436, REAL_BD: 16.988237091926784 GB/s, TOTAL_BD: 4.154605086555923 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
A2A: 17.74534249305725, REAL_BD: 1.7059633301798036 GB/s, TOTAL_BD: 1.6905844455657761 GB/s, comm_vol: 30.0 GB
P2P: 17.985027074813843, REAL_BD: 1.6831426841044699 GB/s, TOTAL_BD: 1.6680542028214056 GB/s, comm_vol: 30.0 GB
AGD: 7.2010931968688965, REAL_BD: 17.04241598026209 GB/s, TOTAL_BD: 4.166034125630298 GB/s, comm_vol: 30.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
A2A: 36.167935848236084, REAL_BD: 1.6743015112373114 GB/s, TOTAL_BD: 1.6589279590564805 GB/s, comm_vol: 60.0 GB
P2P: 35.87622690200806, REAL_BD: 1.6880037841627569 GB/s, TOTAL_BD: 1.6724166720174716 GB/s, comm_vol: 60.0 GB
Traceback (most recent call last):
  File "all2all_test.py", line 98, in <module>
    main()
  File "all2all_test.py", line 90, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 62, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.49 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
AGD: 0.3284647464752197, REAL_BD: 11.995525966762425 GB/s, TOTAL_BD: 2.8541875804340773 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
AGD: 0.55155348777771, REAL_BD: 14.53890672114565 GB/s, TOTAL_BD: 3.3994889735076295 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
AGD: 0.9970376491546631, REAL_BD: 15.970556748115666 GB/s, TOTAL_BD: 3.761141821654811 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
AGD: 1.8993513584136963, REAL_BD: 16.440769003056943 GB/s, TOTAL_BD: 3.9487164745883896 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
AGD: 3.69964861869812, REAL_BD: 16.73850197331508 GB/s, TOTAL_BD: 4.054439095699416 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
AGD: 7.262249946594238, REAL_BD: 16.914296822776812 GB/s, TOTAL_BD: 4.130951181881179 GB/s, comm_vol: 30.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 98, in <module>
    main()
  File "all2all_test.py", line 90, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 62, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 2; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.53 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
A2A: 0.6054770946502686, REAL_BD: 1.614254714131239 GB/s, TOTAL_BD: 1.548365756992859 GB/s, comm_vol: 0.9375 GB
P2P: 0.5593204498291016, REAL_BD: 1.6985236657455427 GB/s, TOTAL_BD: 1.6761411106753739 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
A2A: 1.158026933670044, REAL_BD: 1.6552747405761352 GB/s, TOTAL_BD: 1.6191333253862321 GB/s, comm_vol: 1.875 GB
P2P: 1.1103243827819824, REAL_BD: 1.7128605014071128 GB/s, TOTAL_BD: 1.688695690264928 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
A2A: 2.234149932861328, REAL_BD: 1.701798053506026 GB/s, TOTAL_BD: 1.678490751602014 GB/s, comm_vol: 3.75 GB
P2P: 2.193704843521118, REAL_BD: 1.7254941241874004 GB/s, TOTAL_BD: 1.709436896707066 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
A2A: 4.484463214874268, REAL_BD: 1.6905573352171555 GB/s, TOTAL_BD: 1.6724409679900296 GB/s, comm_vol: 7.5 GB
P2P: 4.463525772094727, REAL_BD: 1.695078303758553 GB/s, TOTAL_BD: 1.6802860301353788 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
A2A: 9.060718774795532, REAL_BD: 1.6724417328839967 GB/s, TOTAL_BD: 1.655497800210502 GB/s, comm_vol: 15.0 GB
P2P: 8.917887449264526, REAL_BD: 1.698920523368003 GB/s, TOTAL_BD: 1.6820127059617775 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
A2A: 17.919157028198242, REAL_BD: 1.690180588567139 GB/s, TOTAL_BD: 1.6741858979633306 GB/s, comm_vol: 30.0 GB
P2P: 18.077109575271606, REAL_BD: 1.6744555301767234 GB/s, TOTAL_BD: 1.659557346548266 GB/s, comm_vol: 30.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
A2A: 35.37633800506592, REAL_BD: 1.711363108075955 GB/s, TOTAL_BD: 1.6960489237582463 GB/s, comm_vol: 60.0 GB
P2P: 35.10501551628113, REAL_BD: 1.7241500084000994 GB/s, TOTAL_BD: 1.7091574841256796 GB/s, comm_vol: 60.0 GB
SIZE: [1, 1024, 1024, 8192], in/out_dim: 2/1
A2A: 72.50486135482788, REAL_BD: 1.6687060244261862 GB/s, TOTAL_BD: 1.6550614366771637 GB/s, comm_vol: 120.0 GB
P2P: 72.77830743789673, REAL_BD: 1.663217280507118 GB/s, TOTAL_BD: 1.6488429619278868 GB/s, comm_vol: 120.0 GB

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
AGD: 0.3229067325592041, REAL_BD: 12.364385701346986 GB/s, TOTAL_BD: 2.903315123131141 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
AGD: 0.566861629486084, REAL_BD: 14.299353091564592 GB/s, TOTAL_BD: 3.3076855134821397 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
AGD: 1.0014328956604004, REAL_BD: 15.88502817419241 GB/s, TOTAL_BD: 3.744634329719159 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
AGD: 1.885094165802002, REAL_BD: 16.5631108158588 GB/s, TOTAL_BD: 3.9785810895070965 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
AGD: 3.7113747596740723, REAL_BD: 16.728902528951316 GB/s, TOTAL_BD: 4.041629038108046 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
AGD: 7.244619607925415, REAL_BD: 16.995891675808974 GB/s, TOTAL_BD: 4.141004169105142 GB/s, comm_vol: 30.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 100, in <module>
    main()
  File "all2all_test.py", line 92, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.55 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 100, in <module>
    main()
  File "all2all_test.py", line 92, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 2; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.53 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 2; 39.42 GiB total capacity; 24.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
AGD: 0.8839826583862305, REAL_BD: 16.016047981019394 GB/s, TOTAL_BD: 3.3937317339196458 GB/s, comm_vol: 3.0 GB
SIZE: [1, 1024, 1024, 8192], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 131, in _gather
    output = torch.empty(output_shape, dtype=tensor.dtype, device=tensor.device)
RuntimeError: CUDA out of memory. Tried to allocate 32.00 GiB (GPU 3; 39.42 GiB total capacity; 8.00 GiB already allocated; 30.55 GiB free; 8.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
A2A: 3.628073215484619, REAL_BD: 1.736324614546012 GB/s, TOTAL_BD: 1.6537703744213308 GB/s, comm_vol: 6.0 GB
P2P: 3.627218246459961, REAL_BD: 1.7460201649922604 GB/s, TOTAL_BD: 1.6541601834562316 GB/s, comm_vol: 6.0 GB
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    a, bandwidth = _all_to_all(a.contiguous(), in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 24.00 GiB already allocated; 6.49 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
A2A: 1.9258127212524414, REAL_BD: 1.6075102669050376 GB/s, TOTAL_BD: 1.5577838732153388 GB/s, comm_vol: 3.0 GB
P2P: 1.9149270057678223, REAL_BD: 1.630445210671373 GB/s, TOTAL_BD: 1.5666393502018106 GB/s, comm_vol: 3.0 GB
AGD: 0.9291868209838867, REAL_BD: 15.20625294001597 GB/s, TOTAL_BD: 3.2286295201899167 GB/s, comm_vol: 3.0 GB

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.55 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
AGD: 0.8871026039123535, REAL_BD: 15.440117283376848 GB/s, TOTAL_BD: 3.3817959577271206 GB/s, comm_vol: 3.0 GB

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 274, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 2; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.53 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
AGD: 0.8789339065551758, REAL_BD: 15.553555569152497 GB/s, TOTAL_BD: 3.41322592930561 GB/s, comm_vol: 3.0 GB

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
AGD: 0.8904318809509277, REAL_BD: 15.318613377614739 GB/s, TOTAL_BD: 3.369151604046545 GB/s, comm_vol: 3.0 GB

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 280, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 24.00 GiB already allocated; 2.55 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 101, in <module>
    main()
  File "all2all_test.py", line 93, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 280, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 3; 39.42 GiB total capacity; 36.00 GiB already allocated; 2.55 GiB free; 36.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

CONFIG:
WORLD_SIZE: 2

round 0:
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 104, in <module>
    main()
  File "all2all_test.py", line 96, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 280, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 24.00 GiB already allocated; 14.56 GiB free; 24.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

CONFIG:
WORLD_SIZE: 2

round 0:
  File "all2all_test.py", line 51
    a = torch.randn(tuple(SSIZE), dtype=torch.float32)
    ^
IndentationError: unexpected indent
CONFIG:
WORLD_SIZE: 2

round 0:
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10377 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10377 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "all2all_test.py", line 104, in <module>
    main()
  File "all2all_test.py", line 96, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 30, in comm_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='all2all_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10377 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10377 (errno: 98 - Address already in use).

CONFIG:
WORLD_SIZE: 2

round 0:
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10377 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10377 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "all2all_test.py", line 104, in <module>
    main()
  File "all2all_test.py", line 96, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 30, in comm_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='all2all_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10377 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10377 (errno: 98 - Address already in use).

CONFIG:
WORLD_SIZE: 2

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
A2A: 0.12780141830444336, REAL_BD: 9.067209735962999 GB/s, TOTAL_BD: 4.890399561225137 GB/s, comm_vol: 0.625 GB
P2P: 0.10032916069030762, REAL_BD: 14.828047935424626 GB/s, TOTAL_BD: 6.229494951415243 GB/s, comm_vol: 0.625 GB
AGD: 0.1460890769958496, REAL_BD: 12.5948838686983 GB/s, TOTAL_BD: 4.278211710638409 GB/s, comm_vol: 0.625 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
A2A: 0.205338716506958, REAL_BD: 9.166506936625067 GB/s, TOTAL_BD: 6.087502743096396 GB/s, comm_vol: 1.25 GB
P2P: 0.13610482215881348, REAL_BD: 17.741077767598966 GB/s, TOTAL_BD: 9.184097816471494 GB/s, comm_vol: 1.25 GB
AGD: 0.2551093101501465, REAL_BD: 13.122297992884688 GB/s, TOTAL_BD: 4.899860374614722 GB/s, comm_vol: 1.25 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
A2A: 0.3258659839630127, REAL_BD: 11.023596580410985 GB/s, TOTAL_BD: 7.671865500032558 GB/s, comm_vol: 2.5 GB
P2P: 0.22601032257080078, REAL_BD: 18.695203403436604 GB/s, TOTAL_BD: 11.061441670288495 GB/s, comm_vol: 2.5 GB
AGD: 0.47181153297424316, REAL_BD: 13.585579731948684 GB/s, TOTAL_BD: 5.298725921853373 GB/s, comm_vol: 2.5 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
A2A: 0.5355381965637207, REAL_BD: 13.209534804168843 GB/s, TOTAL_BD: 9.336402206383196 GB/s, comm_vol: 5.0 GB
P2P: 0.40428829193115234, REAL_BD: 18.974689174141268 GB/s, TOTAL_BD: 12.367412313912537 GB/s, comm_vol: 5.0 GB
AGD: 0.8934323787689209, REAL_BD: 13.927627885131107 GB/s, TOTAL_BD: 5.596394443292512 GB/s, comm_vol: 5.0 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
A2A: 0.9505746364593506, REAL_BD: 14.256293151290672 GB/s, TOTAL_BD: 10.519952475533604 GB/s, comm_vol: 10.0 GB
P2P: 0.7886090278625488, REAL_BD: 18.28962597051591 GB/s, TOTAL_BD: 12.680554808133591 GB/s, comm_vol: 10.0 GB
AGD: 1.9179792404174805, REAL_BD: 13.169338811909137 GB/s, TOTAL_BD: 5.213820769938747 GB/s, comm_vol: 10.0 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
A2A: 1.8602590560913086, REAL_BD: 15.651940266285177 GB/s, TOTAL_BD: 10.751190773409316 GB/s, comm_vol: 20.0 GB
P2P: 1.8918449878692627, REAL_BD: 18.96650657232248 GB/s, TOTAL_BD: 10.571690666118208 GB/s, comm_vol: 20.0 GB
AGD: 7.393937587738037, REAL_BD: 6.295889954499634 GB/s, TOTAL_BD: 2.704918693548024 GB/s, comm_vol: 20.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
A2A: 6.908111810684204, REAL_BD: 15.563738478632564 GB/s, TOTAL_BD: 5.79029423613777 GB/s, comm_vol: 40.0 GB
P2P: 6.6669981479644775, REAL_BD: 18.76604125112512 GB/s, TOTAL_BD: 5.999701681665012 GB/s, comm_vol: 40.0 GB
Traceback (most recent call last):
  File "all2all_test.py", line 104, in <module>
    main()
  File "all2all_test.py", line 96, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 63, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 280, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 1; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

CONFIG:
WORLD_SIZE: 2

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 371, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 2

round 0:
output_rin: tensor([[[ 0,  0],
         [ 0,  0],
         [10, 11],
         [14, 15]]], device='cuda:1')
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
output_rin: tensor([[[0, 1],
         [4, 5],
         [0, 0],
         [0, 0]]], device='cuda:0')
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 372, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 4

round 0:
local_rank 3: output_rin: tensor([[[ 0],
         [ 0],
         [ 0],
         [15]]], device='cuda:3')
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
local_rank 0: output_rin: tensor([[[0],
         [0],
         [0],
         [0]]], device='cuda:0')
local_rank 1: output_rin: tensor([[[0],
         [5],
         [0],
         [0]]], device='cuda:1')
local_rank 2: output_rin: tensor([[[ 0],
         [ 0],
         [10],
         [ 0]]], device='cuda:2')
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 372, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 4

round 0:
local_rank 1: output_rin: tensor([[[0],
         [5],
         [0],
         [0]]], device='cuda:1')
local_rank 2: output_rin: tensor([[[ 0],
         [ 0],
         [10],
         [ 0]]], device='cuda:2')
local_rank 3: output_rin: tensor([[[ 0],
         [ 0],
         [ 0],
         [15]]], device='cuda:3')
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
local_rank 0: output_rin: tensor([[[0],
         [0],
         [0],
         [0]]], device='cuda:0')
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 372, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 4

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
local_rank 0: output_rin: tensor([[[ 0],
         [ 6],
         [11],
         [12]]], device='cuda:0')
local_rank 2: output_rin: tensor([[[ 1],
         [ 6],
         [10],
         [12]]], device='cuda:2')
local_rank 3: output_rin: tensor([[[ 1],
         [ 6],
         [11],
         [15]]], device='cuda:3')
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [11],
         [12]]], device='cuda:1')
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 372, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 4

round 0:
local_rank 3: output_rin: tensor([[[ 0],
         [ 5],
         [10],
         [15]]], device='cuda:3')
local_rank 1: output_rin: tensor([[[ 0],
         [ 5],
         [10],
         [15]]], device='cuda:1')
local_rank 2: output_rin: tensor([[[ 0],
         [ 5],
         [10],
         [15]]], device='cuda:2')
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
local_rank 0: output_rin: tensor([[[ 0],
         [ 5],
         [10],
         [15]]], device='cuda:0')
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 372, in _all_to_all
    assert output_rin.equal(output)
AssertionError

CONFIG:
WORLD_SIZE: 4

round 0:
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 3: output_rin: tensor([[[ 3],
         [ 7],
         [11],
         [15]]], device='cuda:3')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 2: output_rin: tensor([[[ 2],
         [ 6],
         [10],
         [14]]], device='cuda:2')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
local_rank 1: output_rin: tensor([[[ 1],
         [ 5],
         [ 9],
         [13]]], device='cuda:1')
equal !!!
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
local_rank 0: output_rin: tensor([[[ 0],
         [ 4],
         [ 8],
         [12]]], device='cuda:0')
equal !!!
RIN: 0.036031246185302734, REAL_BD: 0.00015209588423459227 GB/s, TOTAL_BD: 13.009541706920054 GB/s, comm_vol: 0.46875 GB
CONFIG:
WORLD_SIZE: 4

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
RIN: 0.1596510410308838, REAL_BD: 9.543287419339505 GB/s, TOTAL_BD: 2.9360910957625537 GB/s, comm_vol: 0.46875 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
RIN: 0.213881254196167, REAL_BD: 10.217290211785016 GB/s, TOTAL_BD: 4.383273342600406 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
RIN: 0.3033638000488281, REAL_BD: 11.808318499924122 GB/s, TOTAL_BD: 6.180697893744106 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
RIN: 0.45789265632629395, REAL_BD: 13.769973096478129 GB/s, TOTAL_BD: 8.189692383552343 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
RIN: 0.7628495693206787, REAL_BD: 15.65073390517856 GB/s, TOTAL_BD: 9.831558280459916 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
RIN: 1.3618171215057373, REAL_BD: 16.53007910851594 GB/s, TOTAL_BD: 11.014694824379035 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
RIN: 2.5387330055236816, REAL_BD: 17.30628214045739 GB/s, TOTAL_BD: 11.816918098408578 GB/s, comm_vol: 30.0 GB

CONFIG:
WORLD_SIZE: 4

round 0:
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
RIN: 0.15208935737609863, REAL_BD: 9.614713151709912 GB/s, TOTAL_BD: 3.082069699596651 GB/s, comm_vol: 0.46875 GB
A2A: 0.3651766777038574, REAL_BD: 1.6139981287100522 GB/s, TOTAL_BD: 1.2836252384664502 GB/s, comm_vol: 0.46875 GB
P2P: 0.36353468894958496, REAL_BD: 1.6214867240966975 GB/s, TOTAL_BD: 1.2894230296273221 GB/s, comm_vol: 0.46875 GB
AGD: 0.20926833152770996, REAL_BD: 13.910171981750693 GB/s, TOTAL_BD: 2.2399471366642554 GB/s, comm_vol: 0.46875 GB
SIZE: [1, 1024, 1024, 128], in/out_dim: 2/1
RIN: 0.209197998046875, REAL_BD: 10.281960709213504 GB/s, TOTAL_BD: 4.481400437636761 GB/s, comm_vol: 0.9375 GB
A2A: 0.658336877822876, REAL_BD: 1.6613002817071205 GB/s, TOTAL_BD: 1.4240429658145815 GB/s, comm_vol: 0.9375 GB
P2P: 0.6546473503112793, REAL_BD: 1.6742780489163978 GB/s, TOTAL_BD: 1.4320687306749607 GB/s, comm_vol: 0.9375 GB
AGD: 0.35442471504211426, REAL_BD: 15.223835279894953 GB/s, TOTAL_BD: 2.6451315616875144 GB/s, comm_vol: 0.9375 GB
SIZE: [1, 1024, 1024, 256], in/out_dim: 2/1
RIN: 0.2997016906738281, REAL_BD: 11.905883824037753 GB/s, TOTAL_BD: 6.25622096353338 GB/s, comm_vol: 1.875 GB
A2A: 1.220411777496338, REAL_BD: 1.7113164758031445 GB/s, TOTAL_BD: 1.5363666875179975 GB/s, comm_vol: 1.875 GB
P2P: 1.2270417213439941, REAL_BD: 1.704190681663438 GB/s, TOTAL_BD: 1.5280654010250678 GB/s, comm_vol: 1.875 GB
AGD: 0.6454958915710449, REAL_BD: 15.766328489166076 GB/s, TOTAL_BD: 2.904743507253807 GB/s, comm_vol: 1.875 GB
SIZE: [1, 1024, 1024, 512], in/out_dim: 2/1
RIN: 0.4650387763977051, REAL_BD: 13.572388522552524 GB/s, TOTAL_BD: 8.063843684280144 GB/s, comm_vol: 3.75 GB
A2A: 2.3749773502349854, REAL_BD: 1.7041890773473702 GB/s, TOTAL_BD: 1.5789624265801807 GB/s, comm_vol: 3.75 GB
P2P: 2.36725115776062, REAL_BD: 1.7088057059571184 GB/s, TOTAL_BD: 1.584115816231108 GB/s, comm_vol: 3.75 GB
AGD: 1.269408941268921, REAL_BD: 15.66514880647072 GB/s, TOTAL_BD: 2.954130759667914 GB/s, comm_vol: 3.75 GB
SIZE: [1, 1024, 1024, 1024], in/out_dim: 2/1
RIN: 0.7760136127471924, REAL_BD: 15.32305525942231 GB/s, TOTAL_BD: 9.66477891212371 GB/s, comm_vol: 7.5 GB
A2A: 4.756572723388672, REAL_BD: 1.6770476045786151 GB/s, TOTAL_BD: 1.576765548673638 GB/s, comm_vol: 7.5 GB
P2P: 4.664226293563843, REAL_BD: 1.7083688480893873 GB/s, TOTAL_BD: 1.607983731481733 GB/s, comm_vol: 7.5 GB
AGD: 2.4819703102111816, REAL_BD: 15.834386316542279 GB/s, TOTAL_BD: 3.02179279467765 GB/s, comm_vol: 7.5 GB
SIZE: [1, 1024, 1024, 2048], in/out_dim: 2/1
RIN: 1.350221872329712, REAL_BD: 16.822619301044874 GB/s, TOTAL_BD: 11.109285301473133 GB/s, comm_vol: 15.0 GB
A2A: 9.578996896743774, REAL_BD: 1.6447378751854391 GB/s, TOTAL_BD: 1.5659259692524807 GB/s, comm_vol: 15.0 GB
P2P: 9.391782522201538, REAL_BD: 1.6787629130860648 GB/s, TOTAL_BD: 1.5971409010527038 GB/s, comm_vol: 15.0 GB
AGD: 8.890523433685303, REAL_BD: 14.585640943046121 GB/s, TOTAL_BD: 1.6871897489372225 GB/s, comm_vol: 15.0 GB
SIZE: [1, 1024, 1024, 4096], in/out_dim: 2/1
RIN: 2.4768264293670654, REAL_BD: 17.684103254854247 GB/s, TOTAL_BD: 12.11227385346751 GB/s, comm_vol: 30.0 GB
A2A: 18.638489246368408, REAL_BD: 1.6847836503917253 GB/s, TOTAL_BD: 1.6095725143520048 GB/s, comm_vol: 30.0 GB
P2P: 18.902947902679443, REAL_BD: 1.6647402772883013 GB/s, TOTAL_BD: 1.587054048630562 GB/s, comm_vol: 30.0 GB
Traceback (most recent call last):
  File "all2all_test.py", line 105, in <module>
    main()
  File "all2all_test.py", line 97, in main
    torch.multiprocessing.spawn(comm_test, nprocs=args.gpus, args=(args.gpus, SIZE, 2, 1, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/all2all_test.py", line 64, in comm_test
    b, bandwidth = _all_to_all(a, in_dim, out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 316, in _all_to_all
    output_ag, bandwidth = _all_to_all_allgather_del(tensor, in_dim, out_dim, method)
  File "/home/yhy/fold/test/utils/comm.py", line 167, in _all_to_all_allgather_del
    gathered, bandwidth = _gather(tensor.contiguous(), dim=out_dim, method=method)
  File "/home/yhy/fold/test/utils/comm.py", line 138, in _gather
    dist.all_gather(list(tensor_list),
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2068, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 2; 39.42 GiB total capacity; 24.00 GiB already allocated; 14.49 GiB free; 24.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

CONFIG:
WORLD_SIZE: 4

round 0:
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
SIZE: [1, 1024, 1024, 64], in/out_dim: 2/1
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
equal !!!
RIN: 0.45549702644348145, REAL_BD: 1.7017649617840402 GB/s, TOTAL_BD: 1.0290956313370423 GB/s, comm_vol: 0.46875 GB
