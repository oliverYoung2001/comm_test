Traceback (most recent call last):
  File "binet_test.py", line 70, in <module>
    main()
  File "binet_test.py", line 67, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
GPUs: 0, 2
GPUs: 0, 1
GPUs: 0, 1
1 in recv
SIZE: [1, 1024, 1024, 64]
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
time: 0.7079601287841797
BD: 7.06254462180912 GB/s
SIZE: [1, 1024, 1024, 128]
0 in send !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
1 in recv
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
0 recv done !!!
time: 1.0069398880004883
BD: 9.931079421093656 GB/s
SIZE: [1, 1024, 1024, 256]
0 in send !!!
0 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
0 send done !!!
0 recv done !!!
0 in send !!!
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
time: 2.010260820388794
BD: 9.948957765655456 GB/s
SIZE: [1, 1024, 1024, 512]
0 in send !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
time: 4.020650386810303
BD: 9.948639188132233 GB/s
SIZE: [1, 1024, 1024, 1024]
0 in send !!!
1 in recv
0 send done !!!
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
0 send done !!!
1 in recv
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
1 in recv
0 recv done !!!
1 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
time: 8.014991760253906
BD: 9.981295351633111 GB/s
SIZE: [1, 1024, 1024, 2048]
0 in send !!!
1 in recv
0 send done !!!
0 recv done !!!
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
1 in recv
0 in send !!!
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
0 in send !!!
1 send done !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
0 recv done !!!
1 send done !!!
0 in send !!!
1 in recv
0 send done !!!
1 recv done !!!
1 send done !!!
0 recv done !!!
0 in send !!!
1 in recv
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
1 send done !!!
0 recv done !!!
1 in recv
0 in send !!!
1 recv done !!!
0 send done !!!
0 recv done !!!
1 send done !!!
1 in recv
1 recv done !!!
1 send done !!!
GPUs: 0, 1
GPUs: 0, 1
SIZE: [1, 1024, 1024, 64]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 0.33914732933044434
BD: 14.742855294986878 GB/s
SIZE: [1, 1024, 1024, 128]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 0.5260519981384277
BD: 19.009527642490873 GB/s
SIZE: [1, 1024, 1024, 256]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 1.0470821857452393
BD: 19.10069741637846 GB/s
SIZE: [1, 1024, 1024, 512]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 2.1059412956237793
BD: 18.993881777769122 GB/s
SIZE: [1, 1024, 1024, 1024]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 4.190762996673584
BD: 19.089602552924124 GB/s
SIZE: [1, 1024, 1024, 2048]
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
time: 8.383100032806396
BD: 19.08601822402888 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 89, in <module>
    main()
  File "binet_test.py", line 86, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 2
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
_ done!
Traceback (most recent call last):
  File "binet_test.py", line 89, in <module>
  File "binet_test.py", line 86, in main
    
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 1; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 3
GPUs: 0, 1
GPUs: 0, 1
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 2
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 3
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 4
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
GPUs: 0, 1
SIZE: [1, 1024, 1024, 64]
time: 0.33971118927001953
BD: 14.718384786630471 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.5234026908874512
BD: 19.10574816312958 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.0446290969848633
BD: 19.14555133274236 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.093467950820923
BD: 19.10705152391494 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 4.189467906951904
BD: 19.09550371951767 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 8.374639511108398
BD: 19.10529996995939 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 1; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 2
SIZE: [1, 1024, 1024, 64]
time: 0.3672456741333008
BD: 13.614864250749834 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.5275952816009521
BD: 18.953922350585998 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.0539698600769043
BD: 18.975874697726816 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.1177306175231934
BD: 18.888143595327662 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 4.218437433242798
BD: 18.964368031056082 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 8.433557748794556
BD: 18.971827165453334 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 3
SIZE: [1, 1024, 1024, 64]
time: 0.33933234214782715
BD: 14.734817106887483 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.5259878635406494
BD: 19.011845506635307 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.0503969192504883
BD: 19.04042141923933 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.103834629058838
BD: 19.012901226886935 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 4.209251403808594
BD: 19.00575478281359 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 8.416772365570068
BD: 19.00966226133207 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 4
SIZE: [1, 1024, 1024, 64]
time: 0.3874995708465576
BD: 12.90324009669653 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.6980481147766113
BD: 14.325660063132167 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.391068458557129
BD: 14.377437628587158 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.784539222717285
BD: 14.365033781411816 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 5.56941032409668
BD: 14.364177775494653 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 11.140763759613037
BD: 14.361672453734665 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 5
SIZE: [1, 1024, 1024, 64]
time: 0.36530065536499023
BD: 13.68735567967774 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.6907525062561035
BD: 14.476965207408742 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.3766894340515137
BD: 14.527604778037128 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.7570464611053467
BD: 14.508279263441691 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 5.514942646026611
BD: 14.506043876564727 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 11.031814336776733
BD: 14.50350732123984 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 90, in <module>
    main()
  File "binet_test.py", line 87, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 6
GPUs: 0, 1
SIZE: [1, 1024, 1024, 64]
time: 0.3717622756958008
BD: 13.44945500627211 GB/s
SIZE: [1, 1024, 1024, 128]
time: 0.5239551067352295
BD: 19.085604608971405 GB/s
SIZE: [1, 1024, 1024, 256]
time: 1.046328067779541
BD: 19.11446382437478 GB/s
SIZE: [1, 1024, 1024, 512]
time: 2.097254753112793
BD: 19.0725518397949 GB/s
SIZE: [1, 1024, 1024, 1024]
time: 4.194952487945557
BD: 19.07053780224799 GB/s
SIZE: [1, 1024, 1024, 2048]
time: 8.393146514892578
BD: 19.0631725201151 GB/s
SIZE: [1, 1024, 1024, 4096]
Traceback (most recent call last):
  File "binet_test.py", line 91, in <module>
    main()
  File "binet_test.py", line 88, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 41, in net_test
    b = torch.empty(SIZE, dtype=torch.float32).cuda()
RuntimeError: CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 39.42 GiB total capacity; 32.00 GiB already allocated; 6.53 GiB free; 32.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

GPUs: 0, 2
GPUs: 0, 1
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 91, in <module>
    main()
  File "binet_test.py", line 88, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 2
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 91, in <module>
    main()
  File "binet_test.py", line 88, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 3
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 91, in <module>
    main()
  File "binet_test.py", line 88, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 4
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "binet_test.py", line 91, in <module>
    main()
  File "binet_test.py", line 88, in main
    torch.multiprocessing.spawn(net_test, nprocs=args.gpus, args=(args.gpus, args))
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/yhy/fold/test/binet_test.py", line 28, in net_test
    dist.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=rank, group_name='net_test')
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 595, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 212, in _tcp_rendezvous_handler
    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)
  File "/home/yhy/.local/miniconda3/envs/fastfold/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 188, in _create_c10d_store
    return TCPStore(
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:10375 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:10375 (errno: 98 - Address already in use).

GPUs: 0, 1
SIZE: [1, 1024, 1024, 4096]
time: 16.516611576080322
BD: 19.374433946453657 GB/s
GPUs: 0, 2
SIZE: [1, 1024, 1024, 4096]
time: 16.800608158111572
BD: 19.046929550910303 GB/s
GPUs: 0, 3
SIZE: [1, 1024, 1024, 4096]
time: 16.8257896900177
BD: 19.01842385382052 GB/s
GPUs: 0, 4
SIZE: [1, 1024, 1024, 4096]
time: 22.203487396240234
BD: 14.412150410848806 GB/s
GPUs: 0, 5
SIZE: [1, 1024, 1024, 4096]
time: 21.893850564956665
BD: 14.615976255551526 GB/s
GPUs: 0, 6
SIZE: [1, 1024, 1024, 4096]
time: 22.610190629959106
BD: 14.152910306558471 GB/s
GPUs: 1, 2
SIZE: [1, 1024, 1024, 4096]
time: 16.721416234970093
BD: 19.137135007187528 GB/s
GPUs: 1, 3
